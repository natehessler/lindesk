zendeskTicketNumber: 4694
zendeskLink: https://sourcegraph.zendesk.com/agent/tickets/4694
title: Frontend / Indexed-search tuning for LSIF?
Application engineer: Jason Harris
Customer: Two Sigma Investments 
Date: Nov 22
Version: ​3.32.0
Deployment: Kubernetes
External Services: n/a
Auth Providers: n/a
Slack Links: https://sourcegraph.slack.com/archives/C01JR51JR5J/p1639156764480000
GitHub Issue Link: N/A
Doc Update Link: N/A
Summary: When Two sigma began automatically generating LSIF files for Java and Python repos, they began noticing an increase in frontend pod restarts, averaging 6 restarts per 24 hours. They also noticed spikes in Grafana @:
Reguarly spiking from 10% to 20% and sometimes up to 60%. They asked for ways to optimize the frontend and indexed-search to make it a bit more stable.
Asked them to visit this endpoint: 
Asked the following questions:
1.  When did you begin generating LSIF files automatically?
2.  I'd like to see if the logs have any info that might point us to why the pods keep restarting. When you get a chance, can you send me the output from the command below? I'm curious to see if the pod is OOMing as a result of the new code-intel activity: 
3.  I'm assuming that you are automatically uploading LSIF by using the method outlined here. Please let me know if that is not the case.
4.  Lastly, am I correct in saying you are using your own python indexer? I seem to recall I ticket I worked on with Justin debugging his indexer. We currently do not recommend using our python indexer.
And these as well:
1.  I know this is a lot, but can you navigate to 
2.  Could you send me the logs for both the frontend-0 , and frontend-internal pods?
3.  Lastly, Are you automating LSIF uploads using GitHub actions, or are you using our experimental built-in automated indexing feature?
None of the answers to the above questions pointed to anything strange except for the her answer to the second question:
    t=2021-12-01T13:56:48+0000 lvl=eror msg="syntax highlighting failed (this is a bug, please report it)" filepath=beta_trunc_full_universe.csv repo_name=/revision= snippet="\"tid,time,sector,beta_spy_trunc,beta_sect_trunc,sigma_eq\\n3,2009-12-29 00:00:00+00\"…" error="http://syntect-server:9238: HSS worker timeout while serving request"
Syntax highlighting wasn't something I had considered because of all the frontend restarts. I was mostly looking on the frontend. The customer agreed and we started making this our main focus. I opened a slack thread in the customer support internal channel which is linked above, where Warren mentioned the following: issue: https://github.com/sourcegraph/sourcegraph/issues/21942, which pointed me to the 
I asked for a list of their resources for the syntect-server, which was under provisioned. I recommended they up their CPU's for requests and limits and they began to note enhanced performance. From the issue i linked just above, the 
    t=2021-12-01T13:56:48+0000 lvl=eror msg="syntax highlighting failed (this is a bug, please report it)" filepath=beta_trunc_full_universe.csv repo_name=/revision= snippet="\"tid,time,sector,beta_spy_trunc,beta_sect_trunc,sigma_eq\\n3,2009-12-29 00:00:00+00\"…" error="http://syntect-server:9238: HSS worker timeout while serving request"
