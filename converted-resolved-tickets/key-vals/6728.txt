zendeskTicketNumber: 6728
zendeskLink: https://sourcegraph.zendesk.com/agent/tickets/6728
title: Timeouts across the board; overloaded CPU
Application engineer: Jason Harris
Customer: Netflix 
Date: Mar 9
Version: 3.37.0â€‹
Deployment: pure-docker
External Services: BITBUCKETSERVER
Auth Providers: http-header
Slack Links: https://netflix.slack.com/archives/CHKL4G5PG/p1646848114303209 and https://netflix.slack.com/archives/CHKL4G5PG/p1646850507952629
GitHub Issue Link: https://github.com/sourcegraph/customer/issues/754\
Doc Update Link:
Summary: Timeouts are occurring across the board, ultimately resulting in needing to shut down the instance to restart it. Timeouts started slowly, then increased rapidly, until the instance was not useable.
Got on a call with Ricardo. Here's the summary:
-   Netflix restarted their instance and it appears to be healthy now.
-   There were some logs that caused concern. They saw some code monitors take 20s+ to run a GraphQL query. The CodeMonitors in question performed diff-level queries against a large monorepo.
-   Zoekt was quite active for a brief period of time that caused some more alerts to fire regarding CPU utilization. I think this is largely expected for a single-instance deployment with a large repo & usage footprint.
-   The Worker Grafana dashboard did not have any data. We believe this is due to an out-of-date Prometheus configuration. This is causing concern as alerts are showing in Granfana indicating the workers were not running. We added debug logging to verify the workers were running.
-   We still want to do a little bit of follow-up work regarding the Discover: failed to connect to host=codeinsights-DB user=postgres database=postgres: server error (FATAL: sorry, too many clients already (SQLSTATE 53300))" error message.
This ultimately ended up being a Prometheus config issue. We updated the Prometheus config and all the deployment is now healthy.
