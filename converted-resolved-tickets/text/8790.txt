-   Application engineer: Michael Bali
-   Customer: Nutanix, Inc. 
-   Date: June 6, 2022
-   Version: 3.34.2​
-   Deployment: kubernetes
-   External Services: GITHUB,OTHER
-   Auth Providers: builtin,saml
-   Slack Links: https://sourcegraph.slack.com/archives/C02BQBNJVL0/p1654574382545949?thread_ts=1653959717.867999&cid=C02BQBNJVL0
-   GitHub Issue Link: https://github.com/sourcegraph/customer/issues/1031
-   Doc Update Link: N/A
The customer is seeing an intermittent spike in memory utilization in its frontend chart. Frontend gets OOMKilled and restarts a lot of times.
-   Due to the slow request, we were seeing in the logs we initially thought this was related to slow code intel searches.
-   Code intel might be running a lot of global searches over all repositories and causing a lot of load on the Sourcegraph instance. Disabling global searches means that jump-to-definition and find-references will never return results from other repositories besides the current one. We disabled code intel global search in the global setting 
-   We monitored the grafana chart and we were still seeing the same spikes and front end getting OOM.
-   There is an impact when you disable code intel for global search. There is an impact, as I said "Disabling global searches means that jump-to-definition and find-references will never return results from other repositories besides the current one." By default, global searches are enabled, and code intel looks for results in other repositories as well.
-   We still see restarts but with a different error in the frontend logs.
-   From the errors in the logs, We think this may be search-related.
-   We noticed you had 2 repo-updater replicas running and told you to scale down the the replicas to 1. This is to prevent any issues as the repo-updater is a singleton service.
-   We noticed the spikes are happening at midnight and we initially thought its a batch job running. To verify this,  We asked if you run a batch search job probably code insight or a cron job.
-   We checked and found no code insight or monitoring running.
-   So we found out in the logs that this slow search comes from a particular user and he  runs them 12:30am UTC everyday.
-   We  enable debug logs by editing the searcher deployment and index-search deployment by adding the below  env variable in the spec section
-   The admin tried to find out what queries this person runs at midnight but no dice.
-   From the debug logs we were able to see a very expensive query (
    awk 'match($0, / MatchCount=[0-9]{1,12}/) {
        print substr($0, RSTART, RLENGTH)
    }
    ' indexed-search-* 
-   We found the same query in the table 
-   Apparently,The customer had code insights disabled after running a trial, however, the jobs were still active.
Based on our findings so far, We believe the root cause is most likely a code insights job targeting the GraphQL API. This is a bug found in v3.38 and code insights moved to the Stream API with v3.40. The timing of the OOMs is consistent with code insights and the logs from webserver and the data in the db. What we have done with the queries above is to disable the series causing this issue, as a workaround.
Code Insights processes some queries in the background of the 
First, find any series with a similar pattern:
    select * from insight_series where query like '%-file%';
Second, disable any of those series:
    update insight_series set deleted_at = current_timestamp where query like '%-file%';
This should stop the Recurring OOM (out of memory) alerts from the 
    select series_id, count(*) from insights_query_runner_jobs where state in ('errored', 'failed')
    group by series_id;
Next, To disable specific series (
    update insight_series set deleted_at = current_timestamp where series_id in ();
To disable all series (
    update insight_series set deleted_at = current_timestamp;
    Last State:  Terminated
          Reason:    OOMKilled
          Message:   t=2022-06-08T02:55:51+0000 lvl=info msg="endpoints: using rendezvous hashing"
    t=2022-06-08T02:55:51+0000 lvl=info msg="Checked current schema state" schema=frontend appliedVersions=[] pendingVersions=[] failedVersions=[]
    t=2022-06-08T02:55:51+0000 lvl=info msg="Schema is in the expected state" schema=frontend
    Jaeger URL from env  http://jaeger-query:16686/
    ✱ Sourcegraph is ready at: https://sourcegraph.canaveral-corp.us-west-2.aws/
    t=2022-06-08T03:00:48+0000 lvl=warn msg="slow search request"
    t=2022-06-08T03:00:52+0000 lvl=eror msg="API HTTP handler error response" method=POST request_uri=/.api/graphql?HighlightedFile status_code=500 error="unexpected EOF" trace= traceID=
    t=2022-06-08T03:00:52+0000 lvl=warn msg="unexpected status code" method=POST url=/.api/graphql?HighlightedFile code=500 duration=51.377353ms x_forwarded_for=127.0.0.1 user=6029
    t=2022-06-08T03:00:59+0000 lvl=warn msg="slow search request"
    t=2022-06-08T03:01:19+0000 lvl=warn msg="slow http request" method=POST url=/.api/graphql?CodeIntelSearch code=200 duration=16.15779419s x_forwarded_for=127.0.0.1 user=6029
    t=2022-06-08T03:01:32+0000 lvl=warn msg="slow http request" method=POST url=/.api/graphql?CodeIntelSearch code=200 duration=23.057623069s x_forwarded_for=127.0.0.1 user=xxx
          Exit Code:    137
          Started:      Wed, 08 Jun 2022 08:25:51 +0530
          Finished:     Wed, 08 Jun 2022 08:33:24 +0530
    t=2022-06-14T01:52:11+0000 lvl=warn msg="returned default value for out of bounds index on highlighted code"
