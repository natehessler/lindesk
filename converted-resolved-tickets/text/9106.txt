-   Application engineer: Stomzy Mwendwa
-   Customer: bet365 
-   Date: June 17, 2022
-   Version: 3.35.1​
-   Deployment: docker-compose
-   External Services: GITLAB
-   Auth Providers: gitlab,gitlab
-   Slack Links: N/A
-   GitHub Issue Link: N/A
-   Doc Update Link: N/A
ust for some context and reference -- my Observability team report the issue as Swap Memory consumption hitting 100% (then drops with no intervention from us) -- e.g.
From a Docker host server perspective we see these corresponding out of memory errors for the postgres container (since raising the memory to 4G to 8G around Jun 10 we saw a lot less):
\[root@xxxxxxx /\]
Jun 15 12:04:52 xxxxxxx kernel: Memory cgroup out of memory: Kill process 19868 (postgres) score 94 or sacrifice child
Jun 16 20:12:27 xxxxxxx kernel: Memory cgroup out of memory: Kill process 9118 (postgres) score 86 or sacrifice child
\[root@xxxxxxx /\]
May 31 12:40:02 xxxxxxx kernel: Memory cgroup out of memory: Kill process 8050 (postgres) score 95 or sacrifice child
Here's the local grafana data:
Jun  2 22:17:31 xxxxxxx kernel: Memory cgroup out of memory: Kill process 27436 (postgres) score 104 or sacrifice child
I was wondering if it would be possible to share pgsql logs from your instance. Particularly, it would be great to pull them between the period where you saw the mem spikes. This could help understand what events where happening prior to and during those spike. I also have a suspicion that repo-updater and gitserver logs can prove helpful as well.  If you're able to pull these for me, I'd highly appreciate that.
In addition, I'd also like to take a look at your postgres config file as well, please share it with me.
Pgsql I took a look at the logs you shared and immediately one thing stuck out whilst looking at pgsql logs. It seems that the database is restarting quite often and this is attributed to OOM issues.
Some context on these logs:
2022-06-15 11:04:53.081 UTC \[1\] LOG:  terminating any other active server processes 2022-06-15 11:04:53.081 UTC \[20633\] WARNING:  terminating connection because of crash of another server process 2022-06-15 11:04:53.081 UTC \[20633\] DETAIL:  The postmaster has commanded this server process to roll back the current transaction and exit, because another server process exited abnormally and possibly corrupted shared memory. 2022-06-15 11:04:53.081 UTC \[20633\] HINT:  In a moment you should be able to reconnect to the database and repeat your command.
These indicate that Postgres detected one of its child processes was externally terminated by the linux kernel OOM killer.
This happens when the container running the database runs low on memory and one of the Postgres backends looks at it and says, "Hmmm..this looks like a sensible process to stop". It's a bit of a protective mechanism if we can call it that.
These logs show that the Postgres server has to actually terminate all active transactions and restart the server to protect itself from data corruption. (If a backend dies, it may be holding onto locks or may have atomically altered some internal structure; so it just rolls back to the last checkpoint for safety). That's what we are seeing here with the "The postmaster has commanded this server process.." detail log.
If I may ask, what resource allocation did you give your database container(s) when you were resizing? The reason I ask is because while we see these, the database is likely under-provisioned on memory and we need to increase it and do a bit of monitoring until we see a plateau.
Gitserver Concerning the gitserver , these logs, lvl=warn msg="slow http request" method=POST url=/search code=200 are indicating that the gitserver is not in great shape resource wise. I'd love to also see what you've provisioned for here please.
Repo-updater I've noticed a few stale permissions which we can always manually queue up for syncing but this shouldn't be a blocker.
Hi Phil,
Thanks for getting back to me so quickly. I appreciate the pgsql tweaks that you made and sorry those didn't go as expected.
Luckily, this phenomenon isn't unique to you. I've seen this with a couple of customers with their instances managed on our infrastructure. I do have some visibility to this so I'm happy to share my observations.
Whenever we see the postgres container getting killed by the linux kernel due to OOMs, the recommended move is to increase the mem_limit. I do apologize that currently, we haven't wrapped our heads around "You need X GB memory for Y number of repos" to help with resource allocation. However, we are working on this this quarter to come up with a much more detailed and accurate resource estimator.
Since you have 8 GB of memory already allocated, I'd suggest probably bumping this up 2X-3X the current and observe behavior. I have a strong hunch that the spikes you saw were related to some expensive queries being run on Postgres causing these child processes to get killed.
We tried experimenting with environment variables from Postgres docs to see if we could make the container un-killable when the spikes happened. This looked like, environment: - 'PG_OOM_ADJUST_FILE=/proc/self/oom_score_adj' - 'PG_OOM_ADJUST_VALUE=0' oom_score_adj: -1000
This unfortunately didn't yield expected results so we've been forced to revert to a "resize memory, monitor and resize again if need arises" type of scenario due to Postgres tuning issues.
As an example, I've seen for an instance with about 1K repos with 40g of memory and another with 10K repos, with a limit of 70g, which is extremely high I must say. This has been part of the resize and monitor principle until we are able to get postgres tuned correctly for its container. There are subtle nuances like the number of monorepos and overall repo sizes to justify why the one with 40g has such high memory. I apologize about this but assure you, work is being done to improve this.
Regarding the postgres config file, this doc serves as a great first step into making custom changes based on your needs and is the knowledge base to use. I'd recommend adding the following customizations. You can paste this directly into your config file since I already tweaked it for you.
effective_cache_size = 6144MB 
shared_buffers = 2048MB 
maintenance_work_mem = x512B 
work_mem = 10.24MB 
I hope this helps.
