
# SAP - Sudden Prometheus OOM <!-- Ticket Title  Hint: include keywords to make it searchable -->

- Zendesk Link: [#5430](https://sourcegraph.zendesk.com/agent/tickets/5430)
- Application engineer: Warren Gifford
- Customer: SAP <!-- Redact if this contains personally identifying information -->
- Date: Jan 6

<!-- Data populated from integration, speak to Ben Gordon or Michael Bali if not working -->
<!-- During Internal team trial, fill missing data manually (we are waiting for all data to sync) -->

## Technical Environment
- Version: 3.33.0​
- Deployment: kubernetes
- External Services: GITHUB
- Auth Providers: builtin,github


## Links
<!-- Data for application engineer manual entry -->
- Slack Links:
- GitHub Issue Link:
- Doc Update Link:

## Summary
### Customer Question
My prometheus pod OOM'd unexpectedly. I added more resources and the issue was resolved but I'm not sure why it failed

### Application Engineer Answer
Hey Kevin, I did some looking into this and what I found is that a spike like this can be caused by recording rules misbehaving - something is exporting something that is too high cardinality (high label dimensions)If you navigate to the grafana charts for prometheus and take a look at the Average prometheus rule group evaluation duration over 10m by rule group chart, do you see anything unusual?

Hey @Kevin Lin I did a little more investigating on this and found that in almost all cases prometheus OOM crashes due to a misconfiguration of resource request limits, or a problems with a ton of data history stacking up. It also seems to be the case that its memory requirements spike really high at startup. I found this article very informative.I also found another method we can use to look at whats going on with the pod besides our grafana charts --You can port forward the prometheus pod to gain access to a nice UI where you can see which metrics are taking the most memory up. Note that kns is an alias on my system and stands for kubectl -n ns-sourcegraph which is the namespace I use on my test cluster
warrengifford@Warrens-MacBook-Pro ~ % kns port-forward svc/prometheus 3080:30090
Forwarding from 127.0.0.1:3080 -> 9090
Forwarding from [::1]:3080 -> 9090
Handling connection for 3080

If this issue comes up again I think the prometheus grafana charts and the port forwarded UI will be great places to gather data. That said I suspect that having granted prometheus more data this shouldn't be an issue again in the future

### Relevant Docs Pages Used/Created
[https://docs.sourcegraph.com/admin/observability/metrics#accessing-prometheus-directly](https://docs.sourcegraph.com/admin/observability/metrics#accessing-prometheus-directly)
[https://engineering.linecorp.com/en/blog/prometheus-container-kubernetes-cluster/](https://engineering.linecorp.com/en/blog/prometheus-container-kubernetes-cluster/)

<!-- Once complete, upload a copy to https://github.com/sourcegraph/support-tools-internal/tree/main/resolved-tickets as a .md file -->
<!-- Name the file 5430.md -->
