
# Reddit: pgsql still receiving connections after moving to external instance <!-- Ticket Title  Hint: include keywords to make it searchable -->

- Zendesk Link: [#7664](https://sourcegraph.zendesk.com/agent/tickets/7664)
- Application engineer: Jason Harris
- Customer: Reddit <!-- Redact if this contains personally identifying information -->
- Date: Apr 14

<!-- Data populated from integration, speak to Ben Gordon or Michael Bali if not working -->
<!-- During Internal team trial, fill missing data manually (we are waiting for all data to sync) -->

## Technical Environment
- Version: 3.36.2​
- Deployment: kubernetes
- External Services: GITHUB
- Auth Providers: builtin,github


## Links
<!-- Data for application engineer manual entry -->
- Slack Links: https://sourcegraph.slack.com/archives/C02BJ8T258D/p1649954115088029 
- GitHub Issue Link: https://github.com/sourcegraph/customer/issues/834
- Doc Update Link: n/a

## Summary
### Description of Customer Issue
The customer moved their production instance onto a hosted Postgres instance a few weeks ago. Yesterday they noticed that the Sourcegraph Postgres dashboard was still reporting that the `pgsql database` still had connections, which would be very unexpected given that their instances shouldn't be connecting to the instance included in the sourcegraph deployment.

### Troubleshooting Steps
- I checked connections on the new Postgres instance... all are working as expected.
- Set `max_connections` on `pgsql` to 0. 
- Restarted prometheus pods.
- Modified Prometheus config map
  ```
  - kubernetes_sd_configs:
    - role: endpoints
      namespaces:
        names:
        - sourcegraph
  ```
- Visited https://<users instance URL>/-debug/grafana/explore and ran the following:
  - `sumby (job,instance,ns)(pg_stat_activity_count{datname!~"template.*|postgres|cloudsql admin"})`
  This revealed that all metrics were only coming from staging.
- Asked customer to run kubectl get clusterrolebinding prometheus -o yaml to see if they run prometheus with a ClusterRole. They do not.
- Changed a mistake in their configmap
```
      kubernetes_sd_configs:
      - role: pod
      - role: endpoints
        namespaces:
            names:
            - sourcegraph
```

changed to: 
```
      kubernetes_sd_configs:
      - role: pod
        namespaces:
            names:
            - sourcegraph
```

- Changes to configMaps were not taking effect for some reason.
- Checked `prometheus` targets by running `kubectl port-forward svc/prometheus 30090:30090`, and then visiting http://localhost:30090/targets.
  - List of targets:
    - `builtin-alertmanager`
    - `builtin-prometheus`
    - `kubernetes-apiservers`
    - `kubernetes-nodes`

### Resolution
Ultimately, the customer was able to figure this out on their own. There were two issues.
- Prometheus scraping was disabled in the entire production deployment because Staging was scraping metrics from Production. Once we re-enabled scraping metrics started appearing in the Production Prometheus.
- They needed to add this config to the Staging Prometheus to prevent it from scraping the production namespace:
```
kubernetes_sd_configs:
  - role: endpoints
    namespaces:
        own_namespace: true
```
This fixed the issue.

### Relevant Docs Pages Used/Created

### Relevant Error / Logs
<!-- Please redact keys, tokens, and personal identifying information -->


<!-- Once complete, upload a copy to https://github.com/sourcegraph/support-tools-internal/tree/main/resolved-tickets as a .md file -->
<!-- Name the file 7664.md -->
