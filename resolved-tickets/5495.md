
# Workiva: pgsql pod restarts <!-- Ticket Title  Hint: include keywords to make it searchable -->

- Zendesk Link: [#5495](https://sourcegraph.zendesk.com/agent/tickets/5495)
- Application engineer: Ben Gordon
- Customer: Workiva <!-- Redact if this contains personally identifying information -->
- Date: Jan 11

<!-- Data populated from integration, speak to Ben Gordon or Michael Bali if not working -->
<!-- During Internal team trial, fill missing data manually (we are waiting for all data to sync) -->

## Technical Environment
- Version: 3.35.1​
- Deployment: kubernetes
- External Services: GITHUB
- Auth Providers: builtin,github


## Links
<!-- Data for application engineer manual entry -->
- Slack Links: 
 - https://sourcegraph.slack.com/archives/CTJUDV6JH/p1640109864034100
 - https://sourcegraph.slack.com/archives/CTJUDV6JH/p1641920610006300
- GitHub Issue Link:
- Doc Update Link:

## Summary
### Description of Customer Issue
Customer reports increase in Sourcegraph instance instability
### Troubleshooting Steps
- The apparent error is pgsql pod restarts on the instance
- We began debugging by collecting pgsql container logs, and inspecting the k8s events on the pgsql deployment
- We tried increasing the memory allocated to pgsql, and collected k8s events and pgsql logs over the course of multiple days
- This showed minimal improvement, but the instance instability remained
- At this point, the customer expressed interest in starting a fresh Sourcegraph instance using our pre-beta Helm installation
- Delivery was brought in, as they were looking for customers to test this deployment option
- Once the customer brought up the new Helm instance, it presented the same PGSQL issues as their previous instance
- At this point, Delivery takes over the debugging, as the cause for the PGSQL failures is due to deployment modifications that the customer has made
- Customer had removed multiple deployments from their instance, including cAdvisor, redisStore, and redisCache
- Adding back in both redis services improves the instance health, but PGSQL issues remain
- Delivery made modifications to the Helm deployment files, including replacing the logic for our Postgres k8s ready checks
- This change prevents k8s from rescheduling pgsql due to errant failed ready checks by changing the ready check logic to be more permissive
- The customer was also experiencing issues with readiness probes, but these appear to have solved themselves with time.
- The customer implements all the same fixes from their Helm deployment into their previous deployment, which resolves all the problems they were experiencing
- Customer begins reporting a new issue with Prometheus failing to start due to an EphemeralStorage limit making the pods unschedulable
- We discover the customer has inserted their own ephemeralStorage limitations on this deployment, so we ask them to remove it
- Removing the ephemeralStorage limitation resolves the unscheduleable pod issue, and Prometheus starts up successfully
- At this point both instances are running in healthy states

### Resolution
- Add back in removed redis pods, redis is necessary for functionality of the Sourcegraph app as it handles auth and sessions
- Remove customer-added ephemeral storage limits, these were too limiting and caused unschedulable pods
- Increase PGSQL memory to accommodate larger data set size, this was causing PGSQL to fail to start due to OOM
- Our Delivery team publishes multiple PR's into our Helm deployment to resolve issues we expect would block any customer's deployments in the future

### Relevant Error / Logs
<!-- Please redact keys, tokens, and personal identifying information -->
Too many to link in a GitHub issue, see linked threads

<!-- Once complete, upload a copy to https://github.com/sourcegraph/support-tools-internal/tree/main/resolved-tickets as a .md file -->
<!-- Name the file 5495.md -->
