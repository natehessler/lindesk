

# Frontend / Indexed-search tuning for LSIF? <!-- Ticket Title  Hint: include keywords to make it searchable -->



- Zendesk Link: [#4694](https://sourcegraph.zendesk.com/agent/tickets/4694)

- Application engineer: Jason Harris

- Customer: Two Sigma Investments <!-- Redact if this contains personally identifying information -->

- Date: Nov 22


<!-- Data populated from integration, speak to Ben Gordon or Michael Bali if not working -->

<!-- During Internal team trial, fill missing data manually (we are waiting for all data to sync) -->



## Technical Environment

- Version: ​3.32.0

- Deployment: Kubernetes

- External Services: n/a

- Auth Providers: n/a





## Links
<!-- Data for application engineer manual entry -->
- Slack Links: https://sourcegraph.slack.com/archives/C01JR51JR5J/p1639156764480000 

- GitHub Issue Link: N/A

- Doc Update Link: N/A



## Summary

### Description of Customer Issue

When Two sigma began automatically generating LSIF files for Java and Python repos, they began noticing an increase in frontend pod restarts, averaging 6 restarts per 24 hours. They also noticed spikes in Grafana @:

`frontend: internal_indexed_search_error_responses: internal indexed search error responses every 5m`

Reguarly spiking from 10% to 20% and sometimes up to 60%. They asked for ways to optimize the frontend and indexed-search to make it a bit more stable.



### Troubleshooting Steps

Asked them to visit this endpoint: `/-/debug/grafana/d/frontend/frontend?orgId=1` on their sg instance and send screen shots of the following dashboards. Nothing suspicious happening here.
`Codeintel: uploadstore stats`
`Codeintel: gitserver client`
`Codeintel: lsifstore stats`

Asked the following questions:

1. When did you begin generating LSIF files automatically?
2. I’d like to see if the logs have any info that might point us to why the pods keep restarting. When you get a chance, can you send me the output from the command below? I’m curious to see if the pod is OOMing as a result of the new code-intel activity: `kubectl logs <podname> -c <containername> —-previous`
3. I’m assuming that you are automatically uploading LSIF by using the method outlined here. Please let me know if that is not the case.
4. Lastly, am I correct in saying you are using your own python indexer? I seem to recall I ticket I worked on with Justin debugging his indexer. We currently do not recommend using our python indexer.


And these as well: 

1. I know this is a lot, but can you navigate to `/-/debug/grafana/d/frontend/frontend?orgId=1` and take screenshots of the following sub-sections:

`Codeintel: Precise Code Intelligence usage at a glance`
`Codeintel: auto-index enqueuer`
`Codeintel: dbstore stats`
`Workerutil: lsif_indexes dbworker/store stats`
`Codeintel: lsifstore stats`
`Codeintel: gitserver client`
`CodeIntel: repo-updater client`
`Codeintel: uploadstore stats`
  
2. Could you send me the logs for both the frontend-0 , and frontend-internal pods?
3. Lastly, Are you automating LSIF uploads using GitHub actions, or are you using our experimental built-in automated indexing feature?


None of the answers to the above questions pointed to anything strange except for the her answer to the second question:

```
t=2021-12-01T13:56:48+0000 lvl=eror msg="syntax highlighting failed (this is a bug, please report it)" filepath=beta_trunc_full_universe.csv repo_name=/revision= snippet="\"tid,time,sector,beta_spy_trunc,beta_sect_trunc,sigma_eq\\n3,2009-12-29 00:00:00+00\"…" error="http://syntect-server:9238: HSS worker timeout while serving request"
```
  
Syntax highlighting wasn't something I had considered because of all the frontend restarts. I was mostly looking on the frontend. The customer agreed and we started making this our main focus. I opened a slack thread in the customer support internal channel which is linked above, where Warren mentioned the following: issue: https://github.com/sourcegraph/sourcegraph/issues/21942, which pointed me to the `syntect-server`.



### Resolution

I asked for a list of their resources for the syntect-server, which was under provisioned. I recommended they up their CPU's for requests and  limits and they began to note enhanced performance. From the issue i linked just above, the `syntect-server` is very underdeveloped and needs work. This will probably be an issue in the future for other large deployments of sourcegraph.



### Relevant Docs Pages Used/Created



### Relevant Error / Logs

<!-- Please redact keys, tokens, and personal identifying information -->

```
t=2021-12-01T13:56:48+0000 lvl=eror msg="syntax highlighting failed (this is a bug, please report it)" filepath=beta_trunc_full_universe.csv repo_name=/revision= snippet="\"tid,time,sector,beta_spy_trunc,beta_sect_trunc,sigma_eq\\n3,2009-12-29 00:00:00+00\"…" error="http://syntect-server:9238: HSS worker timeout while serving request"
```

<!-- Once complete, upload a copy to https://github.com/sourcegraph/support-tools-internal/tree/main/resolved-tickets as a .md file -->
<!-- Name the file 4694.md -->
